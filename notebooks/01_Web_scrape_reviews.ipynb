{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **01 Web Scrape Reviews**\n",
    "This script takes a list of breweries and scrapes the corresponding Tripadvisor review data. The brewery list is sourced from Open Brewery DB.\n",
    "\n",
    "### **Notebook Objectives**\n",
    "1. Import a CSV into pandas and extract a list of breweries to scrape\n",
    "2. For each brewery in the list, use the requests package to query DuckDuckGo and find the corresponding Tripadvisor page (if exists)\n",
    "3. Scrape and save the html page using dill in case the scraped content is needed at a later date\n",
    "4. Save the scraped review data as json files for later processing in the next notebook\n",
    "5. Demonstrate storing the scraped data in a SQL database as an alternative to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import dill\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "from helper import parse_soup\n",
    "\n",
    "logging.basicConfig(filename='scrape.log', filemode='w', \n",
    "        level=logging.DEBUG, force=True,\n",
    "        format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "config = dotenv_values(dotenv_path=Path('../.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = requests.Session()\n",
    "\n",
    "def get_soup(url):\n",
    "    headers = {'User-Agent': config['USER_AGENT']}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # response = s.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup, response\n",
    "\n",
    "def get_brew_id(id, url, review_type):\n",
    "    # check tripadviser page type\n",
    "    if review_type == 'Attraction':\n",
    "        brew_id = re.sub('https://www.tripadvisor.+/Attraction_Review', '-Attraction', url)\n",
    "    elif review_type == 'Restaurant':\n",
    "        brew_id = re.sub('https://www.tripadvisor.+/Restaurant_Review', '-Restaurant', url)\n",
    "    else:\n",
    "        raise TypeError('Unsupported review type')\n",
    "    brew_id = id + brew_id.strip('.html')\n",
    "    return brew_id\n",
    "\n",
    "def save_page(response, page, id):\n",
    "    name = '../assets/html/' + id + page + '.dill'\n",
    "    with open(name, 'wb') as f:\n",
    "        dill.dump(response, f)\n",
    "    return\n",
    "\n",
    "def save_json(id, page, contents):\n",
    "    with open(f'../assets/json/{id}{page}.json', 'w') as f:\n",
    "        json.dump(contents, f)\n",
    "    return\n",
    "\n",
    "def increment_url(url, page):\n",
    "    url = url.replace('-Reviews', '-Reviews' + page)\n",
    "    return url\n",
    "\n",
    "def get_review_type(url):\n",
    "    if 'Attraction_Review' in url:\n",
    "        # scrape up to 30 reviews\n",
    "        pages = ['', '-or10', '-or20']\n",
    "        return 'Attraction', pages\n",
    "    elif 'Restaurant_Review' in url:\n",
    "        # scrape up to 45 reviews since the lengths are truncated in Restaurant case\n",
    "        pages = ['', '-or15', '-or30']\n",
    "        return 'Restaurant', pages\n",
    "    else:\n",
    "        logging.warning('URL does not contain proper format')\n",
    "        return None, None\n",
    "\n",
    "def scrape(id, url_base):\n",
    "    \"\"\"\n",
    "    Inputs: base url (str), brewery name (str)\n",
    "    \"\"\"\n",
    "    # determine if tripadvisor url classifies it as attraction or restaurant\n",
    "    review_type, pages = get_review_type(url_base)\n",
    "    # skip if url does not follow format\n",
    "    if review_type is not None:\n",
    "        # get unique brewery identifier\n",
    "        brew_id = get_brew_id(id, url_base, review_type)\n",
    "        # loop through a couple pages of reviews\n",
    "        review_list = []\n",
    "        for page in pages:\n",
    "            url = increment_url(url_base, page)\n",
    "            logging.info(f'Scraping: {url}')\n",
    "            soup, response = get_soup(url)\n",
    "            save_page(response, page, brew_id)\n",
    "            review_data = parse_soup(soup, id, review_type)\n",
    "            if review_data:\n",
    "                save_json(id, page, review_data)\n",
    "                logging.debug('Page scraped!')\n",
    "                review_list.append(review_data)\n",
    "            time.sleep(random.randint(3,5))\n",
    "    return\n",
    "\n",
    "def get_url_base(name, state, city, verbose=False):\n",
    "    \"\"\"\n",
    "    Inputs: brewery name, state\n",
    "    \"\"\"\n",
    "    base = 'https://duckduckgo.com/html/?q='\n",
    "    name = name.replace(' ', '+')\n",
    "    state = state.replace(' ', '+')\n",
    "    city = city.replace(' ', '+')\n",
    "    url = f'{base}+tripadvisor+{name}+{state}+{city}'\n",
    "    if verbose:\n",
    "        logging.debug(url)\n",
    "    soup, response = get_soup(url)\n",
    "    links = soup.find_all(\"a\", class_=\"result__url\", href=True)\n",
    "    return links[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brewery df shape: (163, 8)\n",
      "Bent Water Brewing Company https://www.tripadvisor.com/Restaurant_Review-g41651-d10214726-Reviews-Bent_Water_Brewing_Company-Lynn_Massachusetts.html\n"
     ]
    }
   ],
   "source": [
    "# Test HTML requests is working\n",
    "df = pd.read_csv(Path('../assets/breweries_clean_address.csv'))\n",
    "states = 'Massachusetts'\n",
    "df_states = df.query('state in @states')\n",
    "breweries_subset = df_states[['obdb_id', 'name', 'state', 'city', 'street', 'longitude', 'latitude', 'website_url']]\n",
    "print(f'Brewery df shape: {breweries_subset.shape}')\n",
    "\n",
    "# Test search for Tripadvisor link\n",
    "test_brew = breweries_subset.iloc[21]\n",
    "link = get_url_base(test_brew['name'], test_brew['state'], \n",
    "    test_brew['city'], verbose=True)\n",
    "print(test_brew['name'], link)\n",
    "\n",
    "# Test scrape\n",
    "time.sleep(1)\n",
    "soup, response = get_soup(link)\n",
    "logging.debug(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180 Commercial St Unit 18, Lynn, MA 01905-2910'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.find('div', class_='_d MJ').text\n",
    "# soup.find('div', class_='wgNTK').find('span').text\n",
    "soup.find('span', class_='yEWoV').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(state):\n",
    "    '''Get list of breweries for a single state'''\n",
    "    df = pd.read_csv(Path('../assets/breweries_clean_address.csv'))\n",
    "    df_subset = df.query('state in @state').reset_index()\n",
    "    columns = ['obdb_id', 'name', 'state', 'city', 'street', 'longitude', 'latitude', 'website_url']\n",
    "    df_subset = df_subset[columns]\n",
    "    return df_subset\n",
    "\n",
    "def scrape_breweries(state, start=0, end=-1):\n",
    "    '''Scrape brewery reviews for datatframe input'''\n",
    "    df_subset = get_subset(state)\n",
    "    count = df_subset.shape[0]-1\n",
    "    # Scrape reviews\n",
    "    for index, brewery in df_subset[start:end].iterrows():\n",
    "        id = brewery['obdb_id']\n",
    "        name = brewery['name']\n",
    "        city = brewery['city']\n",
    "        name_token = re.findall(r'^[A-Za-z\\d]+', name)[0]\n",
    "        state = brewery['state']\n",
    "        url_base = get_url_base(name, state, city)\n",
    "        logging.info(f'Brewery {index} out of {count}')\n",
    "        logging.info(url_base)\n",
    "        if 'tripadvisor.' in url_base and name_token in url_base:\n",
    "            scrape(id, url_base)\n",
    "            # save_json(id, contents)\n",
    "        elif 'tripadvisor.' in url_base:\n",
    "            name_token = name_token.strip('s')\n",
    "            if 'tripadvisor.' in url_base and name_token in url_base:\n",
    "                scrape(id, url_base)\n",
    "                # save_json(id, contents)\n",
    "        else:\n",
    "            logging.info(f'No trip advisor result for {name}')\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163, 8)\n",
      "obdb_id        berkley-beer-company-berkley\n",
      "name                   Berkley Beer Company\n",
      "state                         Massachusetts\n",
      "city                                Berkley\n",
      "street                                  NaN\n",
      "longitude                               NaN\n",
      "latitude                                NaN\n",
      "website_url      http://www.berkleybeer.com\n",
      "Name: 22, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get size of brewery subset\n",
    "state = 'Massachusetts'\n",
    "df_subset = get_subset(state)\n",
    "print(df_subset.shape)\n",
    "print(df_subset.iloc[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape selected state\n",
    "df_scrape = scrape_breweries(state, start=25, end=27)\n",
    "df_scrape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Practice storing the scraped data in a SQL database\n",
    "from sqlalchemy import Column, Date, Integer, String\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "engine = create_engine(\"sqlite:///../assets/foo.db\")\n",
    "Base = declarative_base()\n",
    "\n",
    "class City(Base):\n",
    "\n",
    "    __tablename__ = \"cities\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)  \n",
    "\n",
    "    # def __init__(self, name):\n",
    "    #     self.name = name    \n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Session\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 3\n",
      "New Count: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Count: {session.query(City).count()}')\n",
    "\n",
    "names = ['A', 'B', 'C']\n",
    "ids = ['1', '2', '3']\n",
    "for name, id in zip(names, ids):\n",
    "    if session.query(City).filter(City.id==id).first() is None:\n",
    "        new_city = City(name=name, id=id)\n",
    "        session.add(new_city)\n",
    "\n",
    "# Write to DB\n",
    "session.commit()\n",
    "\n",
    "print(f'New Count: {session.query(City).count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A\n",
      "2 B\n",
      "3 C\n"
     ]
    }
   ],
   "source": [
    "# Inspect table\n",
    "for city in session.query(City).limit(10):\n",
    "    print(city.id, city.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c69408b1de648bf110e3cbae52f18af38dc9daa774b13f8aaf5c2891ede9dc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
