{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **01 Web Scrape Reviews**\n",
    "This script takes a list of breweries and scrapes the corresponding Tripadvisor review data. The brewery list is sourced from Open Brewery DB.\n",
    "\n",
    "### **Notebook Objectives**\n",
    "1. Import a CSV into pandas and extract a list of breweries to scrape\n",
    "2. For each brewery in the list, use the requests package to query DuckDuckGo and find the corresponding Tripadvisor page (if exists)\n",
    "3. Scrape and save the html page using dill in case the scraped content is needed at a later date\n",
    "4. Save the scraped review data into txt files for later processing in the next notebook\n",
    "5. Demonstrate storing the scraped data in a SQL database as an alternative to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import dill\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "logging.basicConfig(filename='scrape.log', filemode='w', \n",
    "        level=logging.DEBUG, force=True,\n",
    "        format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "config = dotenv_values(dotenv_path=Path('../.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = requests.Session()\n",
    "\n",
    "def get_soup(url):\n",
    "    headers = {'User-Agent': config['USER_AGENT']}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # response = s.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup, response\n",
    "\n",
    "def get_brew_id(id, url, review_type):\n",
    "    # check tripadviser page type\n",
    "    if review_type == 'Attraction':\n",
    "        brew_id = re.sub('https://www.tripadvisor.+/Attraction_Review', '-Attraction', url)\n",
    "    elif review_type == 'Restaurant':\n",
    "        brew_id = re.sub('https://www.tripadvisor.+/Restaurant_Review', '-Restaurant', url)\n",
    "    else:\n",
    "        raise TypeError('Unsupported review type')\n",
    "    brew_id = id + brew_id.strip('.html')\n",
    "    return brew_id\n",
    "\n",
    "def save_page(response, page, id):\n",
    "    name = '../assets/html/' + id + page + '.dill'\n",
    "    with open(name, 'wb') as f:\n",
    "        dill.dump(response, f)\n",
    "    return\n",
    "\n",
    "def save_json(id, page, contents):\n",
    "    with open(f'../assets/json/{id}.json', 'w') as f:\n",
    "        json.dump(contents, f)\n",
    "    return\n",
    "\n",
    "def parse_soup(soup, id, type='Attraction'):\n",
    "    # HTML container depends on tripadviser classification\n",
    "    if type == 'Attraction':\n",
    "        # return extract_fields(soup, id, fields)\n",
    "        reviews = soup.find_all('div', {'data-automation': 'reviewCard'})\n",
    "        if len(reviews) == 0:\n",
    "            logging.info('No more reviews to parse.')\n",
    "            return False\n",
    "        # Initialize list for scrapings    \n",
    "        page = []\n",
    "        # Scrape 'about' and 'address' only once per page\n",
    "        about_tag = soup.find('div', class_='_d MJ').text\n",
    "        if about_tag:\n",
    "            logging.debug('Found About section.')\n",
    "            tags = {}\n",
    "            tags['id'] = id\n",
    "            tags['about'] = about_tag\n",
    "            # Scrape 'address' only once per page\n",
    "            address_tag = soup.find('div', class_='wgNTK').find('span').text\n",
    "            if address_tag:\n",
    "                logging.debug('Found Address section.')\n",
    "                tags['address'] = address_tag\n",
    "            page.append(tags)\n",
    "        \n",
    "        # Loop through each review on page\n",
    "        for review in reviews:\n",
    "            # check if tags present\n",
    "            date_tag = review.find('div', class_='RpeCd').text\n",
    "            if date_tag:\n",
    "                tags = {}\n",
    "                tags['id'] = id\n",
    "                tags['date'] = date_tag\n",
    "                tags['rating'] = review.find('svg', class_='UctUV d H0').get('aria-label')\n",
    "                tags['title'] = review.find('div', class_='biGQs _P fiohW qWPrE ncFvv fOtGX').text\n",
    "                # get review text and strip any newlines\n",
    "                text_extract = review.find('div', class_='biGQs _P pZUbB KxBGd').text\n",
    "                text_extract = text_extract.replace('\\n', '')\n",
    "                tags['review'] = text_extract\n",
    "                page.append(tags)\n",
    "        return page\n",
    "\n",
    "    elif type =='Restaurant':\n",
    "        reviews = soup.find_all('div', class_='review-container')\n",
    "        if len(reviews) == 0:\n",
    "            logging.info('No more reviews to parse.')\n",
    "            return False\n",
    "        # Initialize list for scrapings    \n",
    "        page = []\n",
    "        # Scrape 'address' only once per page. 'About' tag not available for restaurants\n",
    "        address_tag = soup.find('span', class_='yEWoV').text\n",
    "        if address_tag:\n",
    "            logging.debug('Found Address section.')\n",
    "            tags = {}\n",
    "            tags['id'] = id\n",
    "            tags['address'] = address_tag\n",
    "        page.append(tags)\n",
    "\n",
    "        # Loop through each review on page\n",
    "        for review in reviews:\n",
    "            # check if tags present\n",
    "            date_tag = review.find('span', class_='ratingDate').get('title')\n",
    "            if date_tag:\n",
    "                tags = {}\n",
    "                tags['id'] = id\n",
    "                tags['date'] = date_tag\n",
    "                tags['rating'] = review.find('span', class_=\"ui_bubble_rating\").get('class')[1]\n",
    "                tags['title'] = review.find('span', class_='noQuotes').text\n",
    "                # get review text and strip any newlines\n",
    "                text_extract = review.find('p', class_='partial_entry').text\n",
    "                text_extract = text_extract.replace('\\n', '')\n",
    "                tags['review'] = text_extract\n",
    "                page.append(tags)\n",
    "        return page\n",
    "    else:\n",
    "        raise TypeError('Unsupported review type')\n",
    "\n",
    "def increment_url(url, page):\n",
    "    url = url.replace('-Reviews', '-Reviews' + page)\n",
    "    return url\n",
    "\n",
    "def get_review_type(url):\n",
    "    if 'Attraction_Review' in url:\n",
    "        # scrape up to 30 reviews\n",
    "        pages = ['', '-or10', '-or20']\n",
    "        return 'Attraction', pages\n",
    "    elif 'Restaurant_Review' in url:\n",
    "        # scrape up to 45 reviews since the lengths are truncated in Restaurant case\n",
    "        pages = ['', '-or15', '-or30']\n",
    "        return 'Restaurant', pages\n",
    "    else:\n",
    "        logging.warning('URL does not contain proper format')\n",
    "        return None, None\n",
    "\n",
    "def scrape(id, url_base):\n",
    "    \"\"\"\n",
    "    Inputs: base url (str), brewery name (str)\n",
    "    \"\"\"\n",
    "    # determine if tripadvisor url classifies it as attraction or restaurant\n",
    "    review_type, pages = get_review_type(url_base)\n",
    "    # skip if url does not follow format\n",
    "    if review_type is not None:\n",
    "        # get unique brewery identifier\n",
    "        brew_id = get_brew_id(id, url_base, review_type)\n",
    "        # loop through a couple pages of reviews\n",
    "        review_list = []\n",
    "        for page in pages:\n",
    "            url = increment_url(url_base, page)\n",
    "            logging.info(f'Scraping: {url}')\n",
    "            soup, response = get_soup(url)\n",
    "            save_page(response, page, brew_id)\n",
    "            review_data = parse_soup(soup, id, review_type)\n",
    "            # conservative delay time between requests\n",
    "            time.sleep(random.randint(4, 6))\n",
    "            if review_data:\n",
    "                logging.debug('Page scraped!')\n",
    "                review_list.append(review_data)\n",
    "            else:\n",
    "                return review_list\n",
    "    return review_list\n",
    "\n",
    "def get_url_base(name, state, city, verbose=False):\n",
    "    \"\"\"\n",
    "    Inputs: brewery name, state\n",
    "    \"\"\"\n",
    "    base = 'https://duckduckgo.com/html/?q='\n",
    "    name = name.replace(' ', '+')\n",
    "    state = state.replace(' ', '+')\n",
    "    city = city.replace(' ', '+')\n",
    "    url = f'{base}+tripadvisor+{name}+{state}+{city}'\n",
    "    if verbose:\n",
    "        logging.debug(url)\n",
    "    soup, response = get_soup(url)\n",
    "    links = soup.find_all(\"a\", class_=\"result__url\", href=True)\n",
    "    return links[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brewery df shape: (163, 8)\n",
      "Bent Water Brewing Company https://www.tripadvisor.com/Restaurant_Review-g41651-d10214726-Reviews-Bent_Water_Brewing_Company-Lynn_Massachusetts.html\n"
     ]
    }
   ],
   "source": [
    "# Test HTML requests is working\n",
    "df = pd.read_csv(Path('../assets/breweries_clean_address.csv'))\n",
    "states = 'Massachusetts'\n",
    "df_states = df.query('state in @states')\n",
    "breweries_subset = df_states[['obdb_id', 'name', 'state', 'city', 'street', 'longitude', 'latitude', 'website_url']]\n",
    "print(f'Brewery df shape: {breweries_subset.shape}')\n",
    "\n",
    "# Test search for Tripadvisor link\n",
    "test_brew = breweries_subset.iloc[21]\n",
    "link = get_url_base(test_brew['name'], test_brew['state'], \n",
    "    test_brew['city'], verbose=True)\n",
    "print(test_brew['name'], link)\n",
    "\n",
    "# Test scrape\n",
    "time.sleep(1)\n",
    "soup, response = get_soup(link)\n",
    "logging.debug(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180 Commercial St Unit 18, Lynn, MA 01905-2910'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.find('div', class_='_d MJ').text\n",
    "# soup.find('div', class_='wgNTK').find('span').text\n",
    "soup.find('span', class_='yEWoV').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(state):\n",
    "    '''Get list of breweries for a single state'''\n",
    "    df = pd.read_csv(Path('../assets/breweries_clean_address.csv'))\n",
    "    df_subset = df.query('state in @state').reset_index()\n",
    "    columns = ['obdb_id', 'name', 'state', 'city', 'street', 'longitude', 'latitude', 'website_url']\n",
    "    df_subset = df_subset[columns]\n",
    "    return df_subset\n",
    "\n",
    "def scrape_breweries(state, start=0, end=-1):\n",
    "    '''Scrape brewery reviews for datatframe input'''\n",
    "    df_subset = get_subset(state)\n",
    "    count = df_subset.shape[0]-1\n",
    "    # Scrape reviews\n",
    "    for index, brewery in df_subset[start:end].iterrows():\n",
    "        id = brewery['obdb_id']\n",
    "        name = brewery['name']\n",
    "        city = brewery['city']\n",
    "        name_token = re.findall(r'^[A-Za-z\\d]+', name)[0]\n",
    "        state = brewery['state']\n",
    "        url_base = get_url_base(name, state, city)\n",
    "        logging.info(f'Brewery {index} out of {count}')\n",
    "        logging.info(url_base)\n",
    "        if 'tripadvisor.' in url_base and name_token in url_base:\n",
    "            contents = scrape(id, url_base)\n",
    "            save_json(id, contents)\n",
    "        elif 'tripadvisor.' in url_base:\n",
    "            name_token = name_token.strip('s')\n",
    "            if 'tripadvisor.' in url_base and name_token in url_base:\n",
    "                contents = scrape(id, url_base)\n",
    "                save_json(id, contents)\n",
    "        else:\n",
    "            logging.info(f'No trip advisor result for {name}')\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163, 8)\n",
      "obdb_id        berkley-beer-company-berkley\n",
      "name                   Berkley Beer Company\n",
      "state                         Massachusetts\n",
      "city                                Berkley\n",
      "street                                  NaN\n",
      "longitude                               NaN\n",
      "latitude                                NaN\n",
      "website_url      http://www.berkleybeer.com\n",
      "Name: 22, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get size of brewery subset\n",
    "state = 'Massachusetts'\n",
    "df_subset = get_subset(state)\n",
    "print(df_subset.shape)\n",
    "print(df_subset.iloc[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape selected state\n",
    "df_scrape = scrape_breweries(state, start=22, end=23)\n",
    "df_scrape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO SANDBOX\n",
    "# If scraping approach needs to be updated, re-scrape from the saved html pages\n",
    "# Extract review data from all txt files in directory\n",
    "dir_path = '../assets/html/'\n",
    "paths = Path(dir_path).glob('**/*.dill')\n",
    "for path in paths:\n",
    "    with open(path, 'rb') as f:\n",
    "        page = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: ../assets/html/amorys-tomb-brewing-co-maynard-Attraction-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.dill\n",
      "Brewery name: amorys\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.tripadvisor.com', port=443): Max retries exceeded with url: /Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x10e691b50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    953\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 954\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x10e691b50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.tripadvisor.com', port=443): Max retries exceeded with url: /Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x10e691b50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mamorys-tomb-brewing-co-maynard-Attraction-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.dill\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     64\u001b[0m url_base \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.tripadvisor.com/Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 65\u001b[0m debug_reviews(path, filename, url_base)\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mdebug_reviews\u001b[0;34m(path, filename, url_base)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBrewery name: \u001b[39m\u001b[39m{\u001b[39;00mbrewery\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m brew_id \u001b[39m=\u001b[39m get_brew_id(brewery, url_base, review_type)\n\u001b[0;32m---> 32\u001b[0m scrape(brew_id, url_base)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m, in \u001b[0;36mscrape\u001b[0;34m(id, url_base)\u001b[0m\n\u001b[1;32m    107\u001b[0m url \u001b[39m=\u001b[39m increment_url(url_base, page)\n\u001b[1;32m    108\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mScraping: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m soup, response \u001b[39m=\u001b[39m get_soup(url)\n\u001b[1;32m    110\u001b[0m save_page(response, page, brew_id)\n\u001b[1;32m    111\u001b[0m review_data \u001b[39m=\u001b[39m parse_soup(soup, brew_id, review_type)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mget_soup\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_soup\u001b[39m(url):\n\u001b[1;32m      4\u001b[0m     headers \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: config[\u001b[39m'\u001b[39m\u001b[39mUSER_AGENT\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m----> 5\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m      6\u001b[0m     \u001b[39m# response = s.get(url, headers=headers)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/main/data science/Data Incubator Bootcamp/capstone/nlp-brewer-finder/.venv_main/lib/python3.9/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.tripadvisor.com', port=443): Max retries exceeded with url: /Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x10e691b50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))"
     ]
    }
   ],
   "source": [
    "# TODO SANDBOX\n",
    "# TODO running parse_soup() will append duplicate reviews to the txt files if they already exist\n",
    "\n",
    "# def debug_reviews(path, filename, url_base):\n",
    "#     with open(path + filename, 'rb') as f:\n",
    "#         print(f'Opening: {path + filename}')\n",
    "#         debug_page = dill.load(f)\n",
    "#     debug_soup = BeautifulSoup(debug_page.content, 'html.parser')\n",
    "#     review_type, pages = get_review_type(url_base)\n",
    "#     # extract brewery name\n",
    "#     brewery = re.findall(r'^[A-Za-z\\d_]+', filename)[0]\n",
    "#     print(f'Brewery name: {brewery}')\n",
    "#     brew_id = get_brew_id(brewery, url_base, review_type)\n",
    "#     print(f'Review type: {review_type}')\n",
    "#     soup_output = debug_soup.find_all('div', class_='_c')\n",
    "#     print(f'Soup result: {soup_output}')\n",
    "#     # print(len(soup_output[0]))\n",
    "#     reviews = parse_soup(debug_soup, brew_id, review_type)\n",
    "#     # print(reviews)\n",
    "#     return\n",
    "\n",
    "def debug_reviews(path, filename, url_base):\n",
    "    with open(path + filename, 'rb') as f:\n",
    "        print(f'Opening: {path + filename}')\n",
    "        debug_page = dill.load(f)\n",
    "    debug_soup = BeautifulSoup(debug_page.content, 'html.parser')\n",
    "    review_type, pages = get_review_type(url_base)\n",
    "    # extract brewery name\n",
    "    brewery = re.findall(r'^[A-Za-z\\d_]+', filename)[0]\n",
    "    print(f'Brewery name: {brewery}')\n",
    "    brew_id = get_brew_id(brewery, url_base, review_type)\n",
    "    scrape(brew_id, url_base)\n",
    "    return\n",
    "\n",
    "path = '../assets/html/'\n",
    "\n",
    "## Case\n",
    "# filename = 'Amorys_Tomb_Brewing_Co-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.dill'\n",
    "# url_base = 'https://www.tripadvisor.com/Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html'\n",
    "# debug_reviews(path, filename, url_base)\n",
    "\n",
    "## Case\n",
    "# filename = 'Amherst_Brewing_Co__Hangar_Pub_and_Grill-g29510-d1067968-Reviews-Hangar_Pub_Grill-Amherst_Hampshire_County_Massachusetts.dill'\n",
    "# url_base = 'https://www.tripadvisor.com/Restaurant_Review-g29510-d1067968-Reviews-Hangar_Pub_Grill-Amherst_Hampshire_County_Massachusetts.html'\n",
    "# debug_reviews(path, filename, url_base)\n",
    "\n",
    "## Case\n",
    "# filename = 'Aquatic_Brewing_LLC-g41565-d22754735-Reviews-Aquatic_Brewing-Falmouth_Cape_Cod_Massachusetts.dill'\n",
    "# url_base = 'https://www.tripadvisor.com/Attraction_Review-g41565-d22754735-Reviews-Aquatic_Brewing-Falmouth_Cape_Cod_Massachusetts.html'\n",
    "# debug_reviews(path, filename, url_base)\n",
    "\n",
    "## CASE\n",
    "# filename = 'Honest_Weight_Artisan_Beer-g41754-d10342647-Reviews-Honest_Weight_Artisan_Beer-Orange_Massachusetts-or10.dill'\n",
    "# url_base = 'https://www.tripadvisor.com/Attraction_Review-g41754-d10342647-Reviews-Honest_Weight_Artisan_Beer-Orange_Massachusetts.html'\n",
    "# debug_reviews(path, filename, url_base)\n",
    "\n",
    "## CASE\n",
    "# filename = 'amorys-tomb-brewing-co-maynard-Attraction-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts-or10.dill'\n",
    "# url_base = 'https://www.tripadvisor.com/Attraction_Review-g41669-d16656179-Reviews-or10-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html'\n",
    "# debug_reviews(path, filename, url_base)\n",
    "\n",
    "## CASE\n",
    "filename = 'amorys-tomb-brewing-co-maynard-Attraction-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.dill'\n",
    "url_base = 'https://www.tripadvisor.com/Attraction_Review-g41669-d16656179-Reviews-Amory_s_Tomb_Brewing-Maynard_Massachusetts.html'\n",
    "debug_reviews(path, filename, url_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Practice storing the scraped data in a SQL database\n",
    "from sqlalchemy import Column, Date, Integer, String\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "engine = create_engine(\"sqlite:///../assets/foo.db\")\n",
    "Base = declarative_base()\n",
    "\n",
    "class City(Base):\n",
    "\n",
    "    __tablename__ = \"cities\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)  \n",
    "\n",
    "    # def __init__(self, name):\n",
    "    #     self.name = name    \n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Session\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 3\n",
      "New Count: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Count: {session.query(City).count()}')\n",
    "\n",
    "names = ['A', 'B', 'C']\n",
    "ids = ['1', '2', '3']\n",
    "for name, id in zip(names, ids):\n",
    "    if session.query(City).filter(City.id==id).first() is None:\n",
    "        new_city = City(name=name, id=id)\n",
    "        session.add(new_city)\n",
    "\n",
    "# Write to DB\n",
    "session.commit()\n",
    "\n",
    "print(f'New Count: {session.query(City).count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A\n",
      "2 B\n",
      "3 C\n"
     ]
    }
   ],
   "source": [
    "# Inspect table\n",
    "for city in session.query(City).limit(10):\n",
    "    print(city.id, city.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c69408b1de648bf110e3cbae52f18af38dc9daa774b13f8aaf5c2891ede9dc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
